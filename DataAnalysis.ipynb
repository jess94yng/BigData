{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jess9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jess9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "uniqueWordsFake={}\n",
    "hashtagsFake={}\n",
    "phrasesFake={}\n",
    "\n",
    "df1 = pd.read_json('ClaimFakeCOVID-19_tweets_05012020.json',lines=True)\n",
    "for index, row in df1.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "    \n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "            \n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json('NewsFakeCOVID-19_tweets_050120.json',lines=True)\n",
    "for index, row in df2.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "                \n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_json('ClaimFakeCOVID-19_tweets_070120.json',lines=True)\n",
    "for index, row in df3.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_json('NewsFakeCOVID-19_tweets_070120.json',lines=True)\n",
    "for index, row in df4.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "    \n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_json('NewsFakeCOVID-19_tweets_090120.json',lines=True)\n",
    "for index, row in df5.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_json('NewsFakeCOVID-19_tweets_110120.json',lines=True)\n",
    "for index, row in df6.iterrows():\n",
    "    tweet = row['full_text']\n",
    "    hashtagList=row['entities']['hashtags']\n",
    "    if len(hashtagList)>0:\n",
    "        for tag in hashtagList:\n",
    "            if tag['text'] in hashtagsFake:\n",
    "                hashtagsFake[tag['text']]+=1\n",
    "            else:\n",
    "                hashtagsFake[tag['text']]=1\n",
    "    unfiltered_sentence=tweet.replace('.','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('!','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('?','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace(',','')\n",
    "    unfiltered_sentence=unfiltered_sentence.replace('\"','')\n",
    "    unfiltered_sentence=unfiltered_sentence.split(' ')\n",
    "    for senIndex in range(len(unfiltered_sentence)):\n",
    "        if senIndex == 0:\n",
    "            phrase= unfiltered_sentence[0]+' '+unfiltered_sentence[1]\n",
    "        elif senIndex == len(unfiltered_sentence)-1:\n",
    "            phrase=unfiltered_sentence[-2]+' '+unfiltered_sentence[-1]\n",
    "        else:\n",
    "            phrase=unfiltered_sentence[senIndex-1]+' '+unfiltered_sentence[senIndex]+' '+unfiltered_sentence[senIndex+1]\n",
    "        \n",
    "        if phrase.lower() in phrasesFake:\n",
    "            phrasesFake[phrase.lower()]+=1\n",
    "        else:\n",
    "            phrasesFake[phrase.lower()]=1\n",
    "\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word.islower() or word.isupper():\n",
    "            if word.lower() in uniqueWordsFake:\n",
    "                uniqueWordsFake[word.lower()]+=1\n",
    "            else:\n",
    "                uniqueWordsFake[word.lower()]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https\n",
      "1 coronavirus\n",
      "2 covid-19\n",
      "3 positive\n",
      "4 confirms\n",
      "5 via\n",
      "6 cdc\n",
      "7 tests\n",
      "8 us\n",
      "9 c\n",
      "10 like\n",
      "11 ban\n",
      "12 greater\n",
      "13 non-essential\n",
      "14 sexual\n",
      "15 persons\n",
      "16 activities\n",
      "17 puts\n",
      "18 indoor\n",
      "19 flu\n",
      "\n",
      "\n",
      "0 coronavirus\n",
      "1 Coronavirus\n",
      "2 COVID19\n",
      "3 Covid_19\n",
      "4 CDC\n",
      "5 China\n",
      "6 SmartNews\n",
      "7 COVIDー19\n",
      "8 COVID\n",
      "9 CoronaVirus\n",
      "10 Trump\n",
      "11 Wuhan\n",
      "12 covid19\n",
      "13 CoronavirusOutbreak\n",
      "14 COVID2019\n",
      "15 FakeNews\n",
      "16 WuhanVirus\n",
      "17 WWG1WGA\n",
      "18 hoax\n",
      "19 CoronavirusPandemic\n",
      "\n",
      "\n",
      "0 positive for coronavirus\n",
      "1   \n",
      "2 tests positive for\n",
      "3 elizabeth tests positive\n",
      "4 queen elizabeth tests\n",
      "5 palace confirms queen\n",
      "6 confirms queen elizabeth\n",
      "7 health minister puts\n",
      "8 on non-essential sexual\n",
      "9 non-essential sexual activities\n",
      "10 sexual activities of\n",
      "11 activities of persons\n",
      "12 of persons 3\n",
      "13 persons 3 or\n",
      "14 3 or greater\n",
      "15 or greater in\n",
      "16 greater in indoor\n",
      "17 minister puts ban\n",
      "18 puts ban on\n",
      "19 ban on non-essential\n"
     ]
    }
   ],
   "source": [
    "uniqueWordsFakePrint=sorted(uniqueWordsFake, key=uniqueWordsFake.get, reverse=True)\n",
    "hashtagsFakePrint=sorted(hashtagsFake,key=hashtagsFake.get,reverse=True)\n",
    "phrasesFakePrint=sorted(phrasesFake,key=phrasesFake.get,reverse=True)\n",
    "for w in range(20):\n",
    "    print(w, uniqueWordsFakePrint[w])\n",
    "print('\\n')\n",
    "for i in range(20):\n",
    "    print(i,hashtagsFakePrint[i])\n",
    "print('\\n')\n",
    "for k in range(20):\n",
    "    print(k,phrasesFakePrint[k])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
